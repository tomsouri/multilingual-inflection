{
  // GENERAL
  "_name_or_path": "bart-base",

  // Whether the model is used as an encoder/decoder or not.
  // (bool, optional, defaults to False)
  "is_encoder_decoder": true,

  // An identifier for the model type, serialized into the JSON file, and used to recreate the correct object in AutoConfig.
  // (str)
  "model_type": "bart",

  // Model architectures that can be used with the model pretrained weights.
  // (List[str], optional)
   "architectures": [

    "BartForConditionalGeneration"
  ],
  // The dtype of the weights. This attribute can be used to initialize the model to a non-default dtype (which is normally float32) and thus allow for optimal storage allocation. For example, if the saved model is float16, ideally we want to load it back using the minimal amount of memory needed to load float16 weights. Since the config object is stored in plain text, this attribute contains just the floating type string without the torch. prefix. For example, for torch.float16 `torch_dtype is the "float16" string. This attribute is currently not being used during model loading time, but this may change in the future versions. But we can already start preparing for the future by saving the dtype with save_pretrained.
  // (str, optional)
  "torch_dtype": "float32",

  "transformers_version": "4.37.2",


  // The non-linear activation function (function or string) in the encoder and pooler. If string, "gelu", "relu", "silu" and "gelu_new" are supported.
  // (str or function, optional, defaults to "gelu")
  "activation_function": "gelu",


  // ===== NORMALIZATION =====
  // DROPOUTS
  // The dropout ratio for activations inside the fully connected layer.
  // (float, optional, defaults to 0.0)
//  "activation_dropout": 0.0,

  // The dropout ratio for the attention probabilities.
  // (float, optional, defaults to 0.0)
//  "attention_dropout": 0.0,

  "classif_dropout": 0.0,

  // The dropout ratio for classifier.
  // (float, optional, defaults to 0.0)
  "classifier_dropout": 0.0,

  // The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.
  // (float, optional, defaults to 0.1)
//  "dropout": 0.0,


  // OTHER
  // The LayerDrop probability for the decoder. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556) for more details.
  // (float, optional, defaults to 0.0)
//  "decoder_layerdrop": 0.0,

  // The LayerDrop probability for the encoder. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556) for more details.
  // (float, optional, defaults to 0.0)
//  "encoder_layerdrop": 0.0,


//  "add_final_layer_norm": false,


  // === MODEL SIZE ===
  // Dimensionality of the layers and the pooler layer.
  // (int, optional, defaults to 1024)
//  "d_model": 256,

  // Number of attention heads for each attention layer in the Transformer decoder.
  // (int, optional, defaults to 16)
//  "decoder_attention_heads": 4,

  // Dimensionality of the “intermediate” (often named feed-forward) layer in decoder.
  // (int, optional, defaults to 4096)
//  "decoder_ffn_dim": 512,

  // Number of decoder layers.
  // (int, optional, defaults to 12)
//  "decoder_layers": 3,

  // Number of attention heads for each attention layer in the Transformer encoder.
  // (int, optional, defaults to 16)
//  "encoder_attention_heads": 4,

  // Dimensionality of the “intermediate” (often named feed-forward) layer in decoder.
  // (int, optional, defaults to 4096)
//  "encoder_ffn_dim": 512,

  // Number of encoder layers.
  // (int, optional, defaults to 12)
//  "encoder_layers": 3,

  // The number of blocks in the model.
  // (int)
//  "num_hidden_layers": 3,



  // TRAINING
  "gradient_checkpointing": false,

  // The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
  // (float, optional, defaults to 0.02)
  "init_std": 0.02,

  "add_bias_logits": false,


  // NORMALIZATION
  // Scale embeddings by diving by sqrt(d_model).
  // (bool, optional, defaults to False)
  // "scale_embedding": false,

  // TODO: try to turn on to avoid divergence
  // "normalize_before": false,

  "normalize_embedding": true,



  // USELESS
  // The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).
  // (int, optional, defaults to 1024)
  "max_position_embeddings": 1024,

  // Whether or not the model should return the last key/values attentions (not used by all models).
  // (bool, optional, defaults to True)
  "use_cache": true,


  // GENERATION CONFIG
  // "num_beams": 1,

  "do_sample": false,

  "no_repeat_ngram_size": 0,

  "early_stopping": false

}
// Missing parameters:
  // Vocabulary size of the BART model. Defines the number of different tokens that can be represented by the inputs_ids passed when calling BartModel or TFBartModel.
  // (defaults to int, optional, defaults to 50265)
  // vocab_size: int, optional, defaults to 50265
  // The number of labels to use in BartForSequenceClassification.
  // (defaults to int, optional, defaults to 3)
  // num_labels: int, optional, defaults to 3
  // The id of the token to force as the last generated token when max_length is reached. Usually set to eos_token_id.
  // (defaults to int, optional, defaults to 2)
  // forced_eos_token_id: int, optional, defaults to 2
  // Store the string that was passed to PreTrainedModel.from_pretrained() or TFPreTrainedModel.from_pretrained() as pretrained_model_name_or_path if the configuration was created with such a method.
  // (defaults to str, optional, defaults to "")
  // name_or_path: str, optional, defaults to ""

  // Whether or not the model should return all hidden-states.
  // (defaults to bool, optional, defaults to False)
  // output_hidden_states: bool, optional, defaults to False

  // Whether or not the model should returns all attentions.
  // (defaults to bool, optional, defaults to False)
  // output_attentions: bool, optional, defaults to False

  // Whether or not the model should return a ModelOutput instead of a plain tuple.
  // (defaults to bool, optional, defaults to True)
  // return_dict: bool, optional, defaults to True

  // Whether the model is used as decoder or not (in which case it’s used as an encoder).
  // (defaults to bool, optional, defaults to False)
  // is_decoder: bool, optional, defaults to False

  // The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder setting and the cross-attention hidden dimension differs from self.config.hidden_size.
  // (defaults to bool, optional)
  // cross_attention_hidden_size**: bool, optional

  // Whether cross-attention layers should be added to the model. Note, this option is only relevant for models that can be used as decoder models within the EncoderDecoderModel class, which consists of all models in AUTO_MODELS_FOR_CAUSAL_LM.
  // (defaults to bool, optional, defaults to False)
  // add_cross_attention: bool, optional, defaults to False

  // Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder and decoder model to have the exact same parameter names.
  // (defaults to bool, optional, defaults to False)
  // tie_encoder_decoder: bool, optional, defaults to False

  // Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of heads to prune in said layer. For instance {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
  // (defaults to Dict[int, List[int]], optional, defaults to {})
  // prune_heads: Dict[int, List[int]], optional, defaults to {}

  // The chunk size of all feed forward layers in the residual attention blocks. A chunk size of 0 means that the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes n < sequence_length embeddings at a time. For more information on feed forward chunking, see How does Feed Forward Chunking work?.
  // (defaults to int, optional, defaults to 0)
  // chunk_size_feed_forward: int, optional, defaults to 0

  // Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow or PyTorch) checkpoint.
  // (defaults to str, optional)
  // finetuning_task: str, optional

  // A map from index (for instance prediction index, or target index) to label.
  // (defaults to Dict[int, str], optional)
  // id2label: Dict[int, str], optional

  // A map from label to index for the model.
  // (defaults to Dict[str, int], optional)
  // label2id: Dict[str, int], optional

  // Number of labels to use in the last layer added to the model, typically for a classification task.
  // (defaults to int, optional)
  // num_labels: int, optional

  // Additional keyword arguments to store for the current task.
  // (defaults to Dict[str, Any], optional)
  // task_specific_params: Dict[str, Any], optional

  // Problem type for XxxForSequenceClassification models. Can be one of "regression", "single_label_classification" or "multi_label_classification".
  // (defaults to str, optional)
  // problem_type: str, optional

  // The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the model by default).
  // (defaults to str, optional)
  // tokenizer_class: str, optional

  // A specific prompt that should be added at the beginning of each text before calling the model.
  // (defaults to str, optional)
  // prefix: str, optional

  // The id of the beginning-of-stream token.
  // (defaults to int, optional)
  // bos_token_id: int, optional

  // The id of the padding token.
  // (defaults to int, optional)
  // pad_token_id: int, optional

  // The id of the end-of-stream token.
  // (defaults to int, optional)
  // eos_token_id: int, optional

  // If an encoder-decoder model starts decoding with a different token than bos, the id of that token.
  // (defaults to int, optional)
  // decoder_start_token_id: int, optional

  // The id of the separation token.
  // (defaults to int, optional)
  // sep_token_id: int, optional

  // Whether or not the model should be used with Torchscript.
  // (defaults to bool, optional, defaults to False)
  // torchscript: bool, optional, defaults to False

  // Whether the model’s input and output word embeddings should be tied. Note that this is only relevant if the model has a output word embedding layer.
  // (defaults to bool, optional, defaults to True)
  // tie_word_embeddings: bool, optional, defaults to True

  // Whether the config class is composed of multiple sub-configs. In this case the config has to be initialized from two or more configs of type PretrainedConfig like: EncoderDecoderConfig or ~RagConfig.
  // (defaults to bool)
  // is_composition: bool

  // A list of keys to ignore by default when looking at dictionary outputs of the model during inference.
  // (defaults to List[str])
  // keys_to_ignore_at_inference: List[str]

  // A dict that maps model specific attribute names to the standardized naming of attributes.
  // (defaults to Dict[str, str])
  // attribute_map: Dict[str, str]

  // A dict that maps sub-modules FQNs of a base model to a tensor parallel plan applied to the sub-module when model.tensor_parallel is called.
  // (defaults to Dict[str, Any])
  // base_model_tp_plan: Dict[str, Any]

  // The number of tokens in the vocabulary, which is also the first dimension of the embeddings matrix (this attribute may be missing for models that don’t have a text modality like ViT).
  // (defaults to int)
  // vocab_size: int

  // The hidden size of the model.
  // (defaults to int)
  // hidden_size: int

  // The number of attention heads used in the multi-head attention layers of the model.
  // (defaults to int)
  // num_attention_heads: int
